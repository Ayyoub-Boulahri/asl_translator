# ğŸ–ï¸ Sign Language Translator

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://www.tensorflow.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![VoiceRSS](https://img.shields.io/badge/TTS-VoiceRSS-lightblue.svg)](https://www.voicerss.org/)

> Real-time sign language to text translator with **camera-based gesture recognition** and **voice output** â€” powered by **MediaPipe**, **TensorFlow**, and **VoiceRSS TTS**.

---

## ğŸŒ Overview

This application translates **sign language gestures** into **text and speech** in real time.  
It uses a trained deep learning model to recognize ASL (American Sign Language) alphabets from webcam input, then converts the detected letters into text â€” and can **speak the text aloud** using online voices from the **VoiceRSS API**.

Built for accessibility, education, and fun exploration of AI-based computer vision.

---

## ğŸš€ Features

âœ… Real-time **gesture recognition** using webcam  
âœ… **Neural network classifier** (TensorFlow) for ASL alphabets  
âœ… **Text-to-Speech** via [VoiceRSS](https://www.voicerss.org/)  
âœ… **Voice selection** (male/female, multiple accents)  
âœ… **Modern dark UI** built with `CustomTkinter`  
âœ… **Alphabet guide window** with all sign images  
âœ… Compatible with **Windows**, **Linux**, and **macOS**

---

## ğŸ“‚ Project Structure

```
asl_translator/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app_view.py          # Main GUI
â”‚   â”œâ”€â”€ app_controller.py    # Links the view and model
â”‚   â”œâ”€â”€ sign_model.py        # Sign detection + VoiceRSS integration
â”‚   â”œâ”€â”€ observer.py          # Observer base class
â”‚   â”œâ”€â”€ subject.py           # Observable base class
â”‚   â”œâ”€â”€ test_main.py         # App entry point
â”‚   â”œâ”€â”€ images/              # Hand sign alphabet reference images
â”‚   â””â”€â”€ ffmpeg/              # (optional) Local FFmpeg binaries
â”‚
â”œâ”€â”€ nn_model/
â”‚   â”œâ”€â”€ asl_nn_model.h5      # Pre-trained TensorFlow model
â”‚   â””â”€â”€ label_encoder.pkl    # Label encoder
â”‚
â”œâ”€â”€ .env                     # API key storage
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/Ayyoub-Boulahri/asl_translator.git
cd asl_translator/src
```

### 2ï¸âƒ£ Create and Activate a Virtual Environment
```bash
python -m venv venv
venv\Scripts\activate   # on Windows
# or
source venv/bin/activate   # on Linux / macOS
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r ../requirements.txt
```

> If you donâ€™t have `requirements.txt`, install manually:
> ```bash
> pip install tensorflow mediapipe customtkinter pillow opencv-python joblib requests playsound python-dotenv
> ```

---

## ğŸ§  Model Setup

Place your pre-trained model files inside the `nn_model/` folder:
```
asl_nn_model.h5
label_encoder.pkl
```

The model predicts ASL letters from MediaPipeâ€™s hand landmark coordinates.

---

## ğŸ”‘ Setup VoiceRSS API

### Step 1 â€” Get Your API Key
- Go to [https://www.voicerss.org/](https://www.voicerss.org/)
- Create a free account and copy your **API key**

### Step 2 â€” Create a `.env` file in the project root
```
VOICERSS_API_KEY=your_api_key_here
```

Your key will be loaded automatically using `python-dotenv`.

---

## ğŸ™ï¸ Optional: FFmpeg Setup

If you donâ€™t have FFmpeg installed globally:

1. Download from [https://ffmpeg.org/download.html](https://ffmpeg.org/download.html)
2. Extract the contents into:
   ```
   src/ffmpeg/bin/ffmpeg.exe
   src/ffmpeg/bin/ffprobe.exe
   ```

---

## ğŸ–¥ï¸ How to Run

```bash
python test_main.py
```

---

## ğŸ§­ Interface Overview

| Button | Description |
|---------|--------------|
| ğŸ¥ **Start Camera** | Starts webcam and begins gesture recognition |
| â¹ **Stop Camera** | Stops the video stream |
| ğŸ§¹ **Clear Text** | Clears translation output |
| ğŸ”Š **Speak Text** | Speaks translated text using selected voice |
| âš™ï¸ **Alphabet Guide** | Opens a window showing all hand sign images |

---

## ğŸ–¼ï¸ Alphabet Guide

When you click the âš™ï¸ button, a new window opens showing all the **sign language alphabet images** from the `/images/` directory â€” each labeled with its corresponding letter.

Example layout:

```
[ A ğŸ–ï¸ ] [ B âœ‹ ] [ C ğŸ¤™ ] ...
```

---

## ğŸ’¡ Notes

- Ensure your **camera is active** and has good lighting.  
- Hand gestures must be **clearly visible** to the webcam.  
- â€œspaceâ€ and â€œdelâ€ gestures let you form words and correct text.  
- All speech playback uses **VoiceRSS**, requiring an internet connection.

---

## ğŸ§± Troubleshooting

| Issue | Cause | Solution |
|--------|--------|-----------|
| `WinError 2` | FFmpeg not found | Add FFmpeg to `src/ffmpeg/bin/` |
| No voice playback | Invalid or missing API key | Check `.env` |
| Black camera feed | Webcam in use elsewhere | Close Zoom/Discord/etc. |
| Wrong gesture prediction | Poor lighting or fast movement | Improve lighting and slow gestures |

---

## ğŸ§© Example TTS Usage

You can use VoiceRSS TTS directly in your own scripts:

```python
from utils import speak_voicerss

speak_voicerss("Hello World!", lang="en-us", voice="John")
```

---

## ğŸ§  Technologies Used

- ğŸ§© **TensorFlow** â€” neural network for gesture recognition  
- âœ‹ **MediaPipe** â€” hand tracking  
- ğŸªŸ **CustomTkinter** â€” modern Python UI  
- ğŸ—£ï¸ **VoiceRSS API** â€” online text-to-speech  
- ğŸ¥ **OpenCV** â€” real-time camera processing  
- ğŸ§® **Joblib** â€” label encoding  
- ğŸ” **Dotenv** â€” API key management  

---

## ğŸª„ Future Improvements

- ğŸŒ Add **multi-language translation** (EN â†” FR)  
- ğŸ—£ï¸ Enable **offline TTS** fallback  
- ğŸ§  Add **model training GUI**  
- ğŸ§ Expand to **full word recognition**

---

## ğŸ‘¤ Author

**Ayyoub Boulahri**  
ğŸ”— [GitHub: Ayyoub-Boulahri](https://github.com/Ayyoub-Boulahri)

---

## ğŸªª License

This project is licensed under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.

---

## ğŸ’¬ Acknowledgments

Special thanks to:
- [MediaPipe](https://developers.google.com/mediapipe) for real-time hand tracking  
- [VoiceRSS](https://www.voicerss.org/) for high-quality TTS  
- [CustomTkinter](https://github.com/TomSchimansky/CustomTkinter) for the beautiful UI framework  

---

> ğŸ§© *â€œBridging communication barriers through intelligent computer vision.â€*
